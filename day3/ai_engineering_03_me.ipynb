{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6-jdBOXowG"
      },
      "source": [
        "# 演習の方針\n",
        "\n",
        "1. **ベースラインモデル評価**  \n",
        "   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n",
        "\n",
        "2. **文字起こしデータの活用**  \n",
        "   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n",
        "\n",
        "3. **チャンク化の導入**  \n",
        "   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n",
        "\n",
        "4. **Rerankの適用**  \n",
        "   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n",
        "\n",
        "5. **応用改善手法**  \n",
        "   文字起こしの品質向上のための編集技術や、メタデータの活用による性能向上手法を探ります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPI1pj4mFavt"
      },
      "source": [
        "## 扱う質問\n",
        "\n",
        "「Inference Time Scaling（推論時スケーリング）」に関する質問を取り扱います。これは以下の背景を持つトピックです。\n",
        "\n",
        "- 2024年8月発表の論文「Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters」で提唱された概念\n",
        "- OpenAIのGPT-o1（2024年9月リリース）で実用化され、注目を集めた比較的新しいアプローチ\n",
        "- 2024年度LLM講座の第4回講義でも取り上げられた重要テーマ\n",
        "\n",
        "## 扱うモデル\n",
        "\n",
        "Meta-Llama-3-8B-Instruct（2024年4月リリース）を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n",
        "\n",
        "- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n",
        "- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 演習環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM50WAI7GXwC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "# !pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vs-Ri1Tslqw"
      },
      "outputs": [],
      "source": [
        "# 演習用のコンテンツを取得（自分のfork）\n",
        "!git clone https://github.com/MasayukiK/lecture-ai-engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [],
      "source": [
        "# モデル(Llama3)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ベースラインモデル評価\n",
        "**まずはベースモデルがどの程度知識を持っているか確かめる**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBUZ3o6dhMlC"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"質問に回答してください。必ず「日本語で回答」すること。\"},\n",
        "    {\"role\": \"user\", \"content\": \"大谷翔平の子供の性別は？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    # max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZXyEnZ3lrBd"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCNnRf9pJif"
      },
      "source": [
        "## 結果 (ベースモデル)\n",
        "\n",
        "Meta-Llama-3-8B-Instructは「Inference Time Scaling」について誤った知識を提示しました：\n",
        "* モデルは従来の「推論時間の短縮」という文脈でInference Time Scalingを解釈しており、これはLLM分野における最新の「Inference Time Scaling」概念（推論時計算資源の最適配分）とは異なる説明になります。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "# 2. 文字起こしデータの活用\n",
        "## 手作業で用意したテキストデータをソースとして活用 (RAG導入)\n",
        "\n",
        "モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n",
        "\n",
        "* **知識ソース**: WikiPedia の大谷翔平のページ（家族について記載した部分のみ抽出）\n",
        "* **目的**: モデルに「大谷翔平」の家族に関する最新の知識を提供し、事実に基づいた回答を促す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPwggQfUS5yl"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/lecture-ai-engineering/day3/data/OtaniSyouhei.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow0wZy6ETHIx"
      },
      "outputs": [],
      "source": [
        "\n",
        "references = raw_writedown\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"質問に回答してください。必ず「日本語で回答」すること。また、与えられる資料を参考にして回答すること。\"},\n",
        "    {\"role\": \"user\", \"content\": f\"[質問] 大谷翔平の子供の性別は？\\n\\n[参考資料]\\n{references}\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    # max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_4dkHGKTPr-"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
